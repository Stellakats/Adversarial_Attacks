{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import requests\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # I'm not sure if it will work out of the box with cuda even if you have a GPU\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "vgg19.eval()\n",
    "resnet.eval()\n",
    "full_vgg19 = nn.Sequential(model, nn.Softmax(dim=1)).to(device).eval()\n",
    "full_resnet = nn.Sequential(model, nn.Softmax(dim=1)).to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is done because the labels for the validation dataset do not match the labels from the pretrained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'labels/imagenet_class_index.json'\n",
    "with open(PATH, 'r') as fp:\n",
    "    labels_65 = json.load(fp)\n",
    "\n",
    "# labels_65 is the default labels from pretrained model\n",
    "labels_65 = {value[0]: int(key) for key,value in labels_65.items()} \n",
    "\n",
    "labels_file = open('labels/labels.txt')\n",
    "labels_490 = {} #labels_490 is the labels from the validation txt\n",
    "for line in labels_file:\n",
    "    contents = line.split()\n",
    "    val = contents[2]\n",
    "    key = contents[1]\n",
    "    n_name = contents[0] \n",
    "    labels_490.update({int(key): [n_name,val]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_labels_file(k_490):\n",
    "    n_name = labels_490[k_490][0]\n",
    "    k_65 = labels_65[n_name]\n",
    "    \n",
    "    return k_65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_handle = open('labels/ILSVRC2012_validation_ground_truth.txt')\n",
    "\n",
    "count = 0\n",
    "num_images = 1000\n",
    "Converted_labels = []\n",
    "for line in ground_truth_handle:\n",
    "    if count == num_images:\n",
    "        break\n",
    "    k_65 = Convert_labels_file(int(line))\n",
    "    Converted_labels.append(k_65)\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __fgsm(model, x, label, epsilon, targeted, clamp):\n",
    "    model.zero_grad()\n",
    "\n",
    "    x = torch.as_tensor(x, device=device)\n",
    "    x.requires_grad = True\n",
    "    \n",
    "    logits = model(x)\n",
    "    target = torch.LongTensor([label]).to(device)\n",
    "    loss = nn.CrossEntropyLoss()(logits, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    diff = epsilon * x.grad.sign()\n",
    "    \n",
    "    if targeted:\n",
    "        adv = x - diff\n",
    "    else:\n",
    "        adv = x + diff\n",
    "    \n",
    "    return adv.clamp(0, 1) if clamp else adv\n",
    "\n",
    "def fgsm_targeted(model, x, label, epsilon, clamp=True):\n",
    "    \"\"\"\n",
    "    model = neural network with logits output (not softmax)\n",
    "    x = image \n",
    "    label = target label (don't care about the true label)\n",
    "    epsilon = distance to move \n",
    "    clamp = Limit output values to the range [0,1]\n",
    "    \"\"\"\n",
    "    return __fgsm(model, x, label, epsilon, True, clamp)\n",
    "    \n",
    "def fgsm_untargeted(model, x, label, epsilon, clamp=True):    \n",
    "    \"\"\"\n",
    "    model = neural network with logits output (not softmax)\n",
    "    x = image\n",
    "    label = the true label\n",
    "    epsilon = distance to move \n",
    "    clamp = Limit output values to the range [0,1]\n",
    "    \"\"\"\n",
    "    return __fgsm(model, x, label, epsilon, False, clamp)\n",
    "\n",
    "def __pgd(model, x, label, k, epsilon, epsilon_step, targeted, clamp):\n",
    "    x = torch.as_tensor(x, device=device)\n",
    "    \n",
    "    x_min = x - epsilon\n",
    "    x_max = x + epsilon\n",
    "    \n",
    "    for i in range(k):\n",
    "        x = __fgsm(model, x.detach(), label, epsilon_step, targeted, clamp)  \n",
    "        x = torch.max(x_min, x)\n",
    "        x = torch.min(x_max, x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def pgd_targeted(model, x, target, k, epsilon, epsilon_step, clamp=False):\n",
    "    \"\"\"\n",
    "    model = neural network (with logits as output - not probs)\n",
    "    x = image\n",
    "    label = target\n",
    "    k = number of iterations \n",
    "    epsilon = maximum allowed distance between input and output in infinity norm\n",
    "    epsilon_step = distance in infinity norm moved each iteration\n",
    "    clamp = bool; limit output channels to the interval [0,1]\n",
    "    \"\"\"\n",
    "    return __pgd(model, x, target, k, epsilon, epsilon_step, True, clamp)\n",
    "\n",
    "def pgd_untargeted(model, x, label, k, epsilon, epsilon_step, clamp=False):\n",
    "    \"\"\"\n",
    "    model = neural network (with logits as output - not probs)\n",
    "    x = image\n",
    "    label = true label\n",
    "    k = number of iterations \n",
    "    epsilon = maximum allowed distance between input and output in infinity norm\n",
    "    epsilon_step = distance in infinity norm moved each iteration\n",
    "    clamp = bool; limit output channels to the interval [0,1]\n",
    "    \"\"\"\n",
    "    return __pgd(model, x, label, k, epsilon, epsilon_step, False, clamp)\n",
    "\n",
    "def plotter(image, adverserial, full_model):\n",
    "    \"\"\"Show the original (img), adversarial attack (adv), and some more stuff. Pass images as pytorch tensors\"\"\"\n",
    "    image = image.cpu().detach()\n",
    "    adverserial = adverserial.cpu().detach()\n",
    "    \n",
    "    probability_original = full_model(image.to(device)).cpu().detach().numpy()\n",
    "    origial_image = image.numpy()[0]\n",
    "    \n",
    "    probability_adverserial = full_model(adverserial.to(device)).cpu().detach().numpy()\n",
    "    adverserial_image = adverserial.numpy()[0]\n",
    "    \n",
    "    f, axes = plt.subplots(1,3, figsize=(15, 6))\n",
    "    \n",
    "    ax = axes[0]\n",
    "    prediction = probability_original.argmax()\n",
    "    ax.set_title(\"Original, class: {} ({:.0f} %): '{}'\".format(\n",
    "        prediction, 100*probability_original[0, prediction], labels[prediction])\n",
    "    )\n",
    "    ax.imshow(np.transpose(origial_image, (1,2,0)))\n",
    "    \n",
    "    ax = axes[1]\n",
    "    prediction = probability_adverserial.argmax()\n",
    "    ax.set_title(\"Adverserial, class: {} ({:.0f} %): '{}'\".format(\n",
    "        prediction, 100*probability_adverserial[0, prediction], labels[prediction])\n",
    "    )\n",
    "    ax.imshow(np.transpose(adverserial_image, (1,2,0)))\n",
    "    \n",
    "    ax = axes[2]\n",
    "    diff = np.transpose(adverserial_image - origial_image, (1,2,0))\n",
    "    plt.imshow(diff + 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 100 validation images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 975 977 978\n",
      "151 620 922 681\n",
      "152 637 637 637\n",
      "153 39 39 39\n",
      "154 115 115 110\n",
      "155 937 937 936\n",
      "156 272 274 274\n",
      "157 277 277 277\n",
      "158 763 763 763\n",
      "159 789 905 736\n",
      "160 646 646 646\n",
      "161 213 214 234\n",
      "162 493 894 789\n",
      "163 647 647 647\n",
      "164 504 504 504\n",
      "165 937 937 937\n",
      "166 687 687 687\n",
      "167 781 781 781\n",
      "168 666 666 666\n",
      "169 583 583 583\n",
      "170 158 158 158\n",
      "171 825 825 825\n",
      "172 212 212 212\n",
      "173 659 659 659\n",
      "174 257 257 257\n",
      "175 436 436 717\n",
      "176 196 199 199\n",
      "177 140 140 140\n",
      "178 248 250 248\n",
      "179 339 339 339\n",
      "180 230 230 230\n",
      "181 361 731 361\n",
      "182 544 926 567\n",
      "183 935 935 961\n",
      "184 638 445 639\n",
      "185 627 656 627\n",
      "186 289 289 285\n",
      "187 867 867 867\n",
      "188 272 272 274\n",
      "189 103 979 885\n",
      "190 584 823 584\n",
      "191 180 179 243\n",
      "192 703 448 644\n",
      "193 449 449 975\n",
      "194 771 771 771\n",
      "195 118 118 118\n",
      "196 396 396 396\n",
      "197 934 922 438\n",
      "198 16 16 16\n",
      "199 548 548 548\n",
      "200 993 993 993\n",
      "201 704 704 704\n",
      "202 457 823 457\n",
      "203 233 233 233\n",
      "204 401 543 747\n",
      "205 827 482 481\n",
      "206 376 376 382\n",
      "207 146 85 362\n",
      "208 606 606 811\n",
      "209 922 922 916\n",
      "210 516 431 431\n",
      "211 284 284 284\n",
      "212 889 889 889\n",
      "213 475 475 475\n",
      "214 978 723 978\n",
      "215 475 475 475\n",
      "216 984 984 984\n",
      "217 16 16 16\n",
      "218 77 77 77\n",
      "219 610 610 610\n",
      "220 254 254 243\n",
      "221 636 636 636\n",
      "222 662 662 662\n",
      "223 473 473 473\n",
      "224 213 207 215\n",
      "225 25 25 25\n",
      "226 463 427 522\n",
      "227 215 215 215\n",
      "228 173 537 537\n",
      "229 35 35 666\n",
      "230 741 741 741\n",
      "231 125 125 363\n",
      "232 787 562 849\n",
      "233 289 289 289\n",
      "234 425 425 425\n",
      "235 973 973 973\n",
      "236 1 115 644\n",
      "237 167 167 167\n",
      "238 121 121 121\n",
      "239 445 562 876\n",
      "240 702 422 702\n",
      "241 532 532 532\n",
      "242 366 294 106\n",
      "243 678 678 678\n",
      "244 764 650 486\n",
      "245 125 113 70\n",
      "246 349 348 349\n",
      "247 13 13 13\n",
      "248 179 179 179\n",
      "249 522 522 731\n",
      "Original_Accuracy: 66.0 %\n",
      "Adversarial_Accuracy: 60.0 %\n",
      "150 975 977 978\n",
      "151 620 922 681\n",
      "152 637 637 637\n",
      "153 39 39 39\n",
      "154 115 115 115\n",
      "155 937 937 108\n",
      "156 272 274 274\n",
      "157 277 277 277\n",
      "158 763 763 763\n",
      "159 789 905 736\n",
      "160 646 646 646\n",
      "161 213 214 234\n",
      "162 493 894 789\n",
      "163 647 647 647\n",
      "164 504 504 504\n",
      "165 937 937 937\n",
      "166 687 687 687\n",
      "167 781 781 781\n",
      "168 666 666 666\n",
      "169 583 583 583\n",
      "170 158 158 158\n",
      "171 825 825 825\n",
      "172 212 212 212\n",
      "173 659 659 659\n",
      "174 257 257 257\n",
      "175 436 436 717\n",
      "176 196 199 199\n",
      "177 140 140 140\n",
      "178 248 250 248\n",
      "179 339 339 339\n",
      "180 230 230 230\n",
      "181 361 731 361\n",
      "182 544 926 567\n",
      "183 935 935 961\n",
      "184 638 445 639\n",
      "185 627 656 627\n",
      "186 289 289 282\n",
      "187 867 867 867\n",
      "188 272 272 280\n",
      "189 103 979 885\n",
      "190 584 823 584\n",
      "191 180 179 243\n",
      "192 703 448 587\n",
      "193 449 449 975\n",
      "194 771 771 771\n",
      "195 118 118 118\n",
      "196 396 396 396\n",
      "197 934 922 951\n",
      "198 16 16 16\n",
      "199 548 548 548\n",
      "200 993 993 993\n",
      "201 704 704 704\n",
      "202 457 823 457\n",
      "203 233 233 183\n",
      "204 401 543 683\n",
      "205 827 482 481\n",
      "206 376 376 382\n",
      "207 146 85 362\n",
      "208 606 606 838\n",
      "209 922 922 922\n",
      "210 516 431 431\n",
      "211 284 284 284\n",
      "212 889 889 889\n",
      "213 475 475 475\n",
      "214 978 723 978\n",
      "215 475 475 475\n",
      "216 984 984 984\n",
      "217 16 16 16\n",
      "218 77 77 77\n",
      "219 610 610 610\n",
      "220 254 254 243\n",
      "221 636 636 636\n",
      "222 662 662 405\n",
      "223 473 473 473\n",
      "224 213 207 215\n",
      "225 25 25 25\n",
      "226 463 427 522\n",
      "227 215 215 215\n",
      "228 173 537 537\n",
      "229 35 35 910\n",
      "230 741 741 741\n",
      "231 125 125 363\n",
      "232 787 562 849\n",
      "233 289 289 289\n",
      "234 425 425 425\n",
      "235 973 973 973\n",
      "236 1 115 644\n",
      "237 167 167 167\n",
      "238 121 121 121\n",
      "239 445 562 876\n",
      "240 702 422 702\n",
      "241 532 532 532\n",
      "242 366 294 106\n",
      "243 678 678 678\n",
      "244 764 650 486\n",
      "245 125 113 125\n",
      "246 349 348 349\n",
      "247 13 13 13\n",
      "248 179 179 179\n",
      "249 522 522 731\n",
      "Original_Accuracy: 66.0 %\n",
      "Adversarial_Accuracy: 61.0 %\n",
      "150 975 977 978\n",
      "151 620 922 681\n",
      "152 637 637 543\n",
      "153 39 39 39\n",
      "154 115 115 115\n",
      "155 937 937 108\n",
      "156 272 274 274\n",
      "157 277 277 277\n",
      "158 763 763 763\n",
      "159 789 905 736\n",
      "160 646 646 646\n",
      "161 213 214 244\n",
      "162 493 894 789\n",
      "163 647 647 438\n",
      "164 504 504 504\n",
      "165 937 937 937\n",
      "166 687 687 687\n",
      "167 781 781 498\n",
      "168 666 666 968\n",
      "169 583 583 583\n",
      "170 158 158 158\n",
      "171 825 825 825\n",
      "172 212 212 177\n",
      "173 659 659 659\n",
      "174 257 257 222\n",
      "175 436 436 717\n",
      "176 196 199 199\n",
      "177 140 140 140\n",
      "178 248 250 249\n",
      "179 339 339 339\n",
      "180 230 230 231\n",
      "181 361 731 361\n",
      "182 544 926 567\n",
      "183 935 935 961\n",
      "184 638 445 639\n",
      "185 627 656 627\n",
      "186 289 289 282\n",
      "187 867 867 867\n",
      "188 272 272 280\n",
      "189 103 979 223\n",
      "190 584 823 584\n",
      "191 180 179 243\n",
      "192 703 448 587\n",
      "193 449 449 975\n",
      "194 771 771 771\n",
      "195 118 118 124\n",
      "196 396 396 396\n",
      "197 934 922 951\n",
      "198 16 16 16\n",
      "199 548 548 548\n",
      "200 993 993 993\n",
      "201 704 704 704\n",
      "202 457 823 457\n",
      "203 233 233 183\n",
      "204 401 543 416\n",
      "205 827 482 481\n",
      "206 376 376 382\n",
      "207 146 85 362\n",
      "208 606 606 419\n",
      "209 922 922 922\n",
      "210 516 431 431\n",
      "211 284 284 284\n",
      "212 889 889 889\n",
      "213 475 475 475\n",
      "214 978 723 978\n",
      "215 475 475 475\n",
      "216 984 984 984\n",
      "217 16 16 16\n",
      "218 77 77 77\n",
      "219 610 610 419\n",
      "220 254 254 243\n",
      "221 636 636 636\n",
      "222 662 662 700\n",
      "223 473 473 473\n",
      "224 213 207 215\n",
      "225 25 25 25\n",
      "226 463 427 522\n",
      "227 215 215 162\n",
      "228 173 537 537\n",
      "229 35 35 958\n",
      "230 741 741 741\n",
      "231 125 125 363\n",
      "232 787 562 849\n",
      "233 289 289 289\n",
      "234 425 425 425\n",
      "235 973 973 973\n",
      "236 1 115 644\n",
      "237 167 167 167\n",
      "238 121 121 121\n",
      "239 445 562 876\n",
      "240 702 422 702\n",
      "241 532 532 532\n",
      "242 366 294 106\n",
      "243 678 678 678\n",
      "244 764 650 486\n",
      "245 125 113 314\n",
      "246 349 348 349\n",
      "247 13 13 13\n",
      "248 179 179 179\n",
      "249 522 522 731\n",
      "Original_Accuracy: 66.0 %\n",
      "Adversarial_Accuracy: 49.0 %\n",
      "150 975 977 978\n",
      "151 620 922 681\n",
      "152 637 637 543\n",
      "153 39 39 39\n",
      "154 115 115 115\n",
      "155 937 937 108\n",
      "156 272 274 277\n",
      "157 277 277 277\n",
      "158 763 763 763\n",
      "159 789 905 736\n",
      "160 646 646 646\n",
      "161 213 214 244\n",
      "162 493 894 789\n",
      "163 647 647 438\n",
      "164 504 504 441\n",
      "165 937 937 937\n",
      "166 687 687 687\n",
      "167 781 781 498\n",
      "168 666 666 968\n",
      "169 583 583 583\n",
      "170 158 158 158\n",
      "171 825 825 825\n",
      "172 212 212 177\n",
      "173 659 659 659\n",
      "174 257 257 222\n",
      "175 436 436 717\n",
      "176 196 199 199\n",
      "177 140 140 140\n",
      "178 248 250 249\n",
      "179 339 339 339\n",
      "180 230 230 231\n",
      "181 361 731 361\n",
      "182 544 926 567\n",
      "183 935 935 961\n",
      "184 638 445 639\n",
      "185 627 656 627\n",
      "186 289 289 282\n",
      "187 867 867 867\n",
      "188 272 272 280\n",
      "189 103 979 223\n",
      "190 584 823 584\n",
      "191 180 179 243\n",
      "192 703 448 587\n",
      "193 449 449 975\n",
      "194 771 771 771\n",
      "195 118 118 124\n",
      "196 396 396 396\n",
      "197 934 922 951\n",
      "198 16 16 16\n",
      "199 548 548 548\n",
      "200 993 993 265\n",
      "201 704 704 704\n",
      "202 457 823 457\n",
      "203 233 233 183\n",
      "204 401 543 642\n",
      "205 827 482 481\n",
      "206 376 376 382\n",
      "207 146 85 362\n",
      "208 606 606 419\n",
      "209 922 922 922\n",
      "210 516 431 431\n",
      "211 284 284 284\n",
      "212 889 889 889\n",
      "213 475 475 475\n",
      "214 978 723 978\n",
      "215 475 475 475\n",
      "216 984 984 984\n",
      "217 16 16 16\n",
      "218 77 77 77\n",
      "219 610 610 419\n",
      "220 254 254 243\n",
      "221 636 636 636\n",
      "222 662 662 700\n",
      "223 473 473 473\n",
      "224 213 207 215\n",
      "225 25 25 25\n",
      "226 463 427 522\n",
      "227 215 215 162\n",
      "228 173 537 537\n",
      "229 35 35 958\n",
      "230 741 741 741\n",
      "231 125 125 363\n",
      "232 787 562 849\n",
      "233 289 289 289\n",
      "234 425 425 425\n",
      "235 973 973 973\n",
      "236 1 115 644\n",
      "237 167 167 167\n",
      "238 121 121 121\n",
      "239 445 562 876\n"
     ]
    }
   ],
   "source": [
    "min_img_size = 224 \n",
    "fro = 150\n",
    "imgs =  100 # Choose number of images you want to check\n",
    "\n",
    "epsilons = [0.01, 0.05, 0.10, 0.20, 0.30]\n",
    "epsilon_step = 0.025\n",
    "k = 5\n",
    "\n",
    "adv_accuracies = []\n",
    "\n",
    "# Choose model\n",
    "model = vgg19\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    \n",
    "    original_correct = 0\n",
    "    adv_correct = 0\n",
    "    cls = {}\n",
    "    \n",
    "    for i in range(fro,fro+imgs):\n",
    "        if(i<10):\n",
    "            directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' +'00' +str(i) \n",
    "        elif(i<100):\n",
    "            directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' +'0' +str(i)\n",
    "        elif(i<1000):\n",
    "            directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' + str(i)\n",
    "        filename = directory_imgs \n",
    "        img = Image.open(directory_imgs+'.JPEG')\n",
    "\n",
    "        # Transformation of image\n",
    "        pre_transform = transforms.Compose([transforms.Resize((min_img_size,min_img_size)), transforms.ToTensor(),transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ])\n",
    "        image_tensor = pre_transform(img)\n",
    "        image_tensor = image_tensor.unsqueeze(0) # add batch dimension.  C X H X W ==> B X C X H X W\n",
    "        img_variable = Variable(image_tensor, requires_grad=True) #convert tensor into a variable\n",
    "\n",
    "        # Classification\n",
    "        output = model.forward(img_variable)\n",
    "        original_class = torch.max(output.data, 1)[1][0]  #get an index(class number) of a largest element\n",
    "        original_class = original_class.item()\n",
    "        cls.update({i:original_class})\n",
    "\n",
    "        # Adversarial attack\n",
    "        #adv = fgsm_untargeted(model, image_tensor, label=original_class, epsilon = epsilon, clamp=True)\n",
    "        adv = pgd_untargeted(model, image_tensor, original_class, k, epsilon, epsilon_step, clamp=False)\n",
    "        adv = adv.cpu().detach()\n",
    "        probability_adverserial = full_vgg19(adv.to(device)).cpu().detach().numpy()\n",
    "        adverserial_image = adv.numpy()[0]\n",
    "        prediction = probability_adverserial.argmax()\n",
    "\n",
    "        # Computing Accuracies\n",
    "        if(original_class == Converted_labels[i-1]):\n",
    "            original_correct+=1\n",
    "        if(prediction == Converted_labels[i-1]):\n",
    "            adv_correct+=1\n",
    "\n",
    "        print(i,Converted_labels[i-1],original_class, prediction)\n",
    "\n",
    "\n",
    "    original_correct = original_correct/imgs*100 \n",
    "    adv_correct = adv_correct/imgs*100\n",
    "    print('Original_Accuracy: '+ str(original_correct) + ' %' )\n",
    "    print('Adversarial_Accuracy: '+ str(adv_correct) + str(' %'))\n",
    "    \n",
    "    adv_accuracies.append(adv_correct)\n",
    "\n",
    "epsilons.insert(0,0)\n",
    "adv_accuracies.insert(0,original_correct)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-model attack\n",
    "Train adversarial attack on Resnet, attack VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 975 978 977\n",
      "151 620 681 549\n",
      "152 637 637 620\n",
      "153 39 39 42\n",
      "154 115 115 110\n",
      "155 937 937 111\n",
      "156 272 274 209\n",
      "157 277 277 277\n",
      "158 763 763 763\n",
      "159 789 579 720\n",
      "160 646 646 646\n",
      "161 213 234 163\n",
      "162 493 894 708\n",
      "163 647 647 551\n",
      "164 504 504 504\n",
      "165 937 937 973\n",
      "166 687 687 699\n",
      "167 781 781 921\n",
      "168 666 666 419\n",
      "169 583 583 583\n",
      "170 158 158 238\n",
      "171 825 825 700\n",
      "172 212 212 212\n",
      "173 659 659 659\n",
      "174 257 257 257\n",
      "175 436 436 468\n",
      "176 196 199 199\n",
      "177 140 140 142\n",
      "178 248 248 248\n",
      "179 339 339 794\n",
      "180 230 231 255\n",
      "181 361 361 862\n",
      "182 544 909 659\n",
      "183 935 935 961\n",
      "184 638 445 921\n",
      "185 627 656 831\n",
      "186 289 289 794\n",
      "187 867 867 908\n",
      "188 272 272 549\n",
      "189 103 103 632\n",
      "190 584 710 813\n",
      "191 180 180 243\n",
      "192 703 703 693\n",
      "193 449 449 978\n",
      "194 771 771 844\n",
      "195 118 118 78\n",
      "196 396 396 78\n",
      "197 934 711 419\n",
      "198 16 16 128\n",
      "199 548 548 405\n",
      "200 993 993 78\n",
      "201 704 704 794\n",
      "202 457 457 723\n",
      "203 233 233 295\n",
      "204 401 543 419\n",
      "205 827 528 605\n",
      "206 376 376 111\n",
      "207 146 82 913\n",
      "208 606 606 419\n",
      "209 922 922 921\n",
      "210 516 431 516\n",
      "211 284 284 264\n",
      "212 889 889 432\n",
      "213 475 475 405\n",
      "214 978 978 395\n",
      "215 475 475 419\n",
      "216 984 984 984\n",
      "217 16 16 313\n",
      "218 77 77 78\n",
      "219 610 610 419\n",
      "220 254 254 163\n",
      "221 636 636 636\n",
      "222 662 662 693\n",
      "223 473 473 419\n",
      "224 213 207 226\n",
      "225 25 25 650\n",
      "226 463 522 463\n",
      "227 215 215 247\n",
      "228 173 537 273\n",
      "229 35 35 727\n",
      "230 741 741 646\n",
      "231 125 125 908\n",
      "232 787 849 980\n",
      "233 289 289 794\n",
      "234 425 425 646\n",
      "235 973 973 950\n",
      "236 1 115 749\n",
      "237 167 167 167\n",
      "238 121 121 395\n",
      "239 445 876 34\n",
      "240 702 422 416\n",
      "241 532 532 428\n",
      "242 366 294 354\n",
      "243 678 678 678\n",
      "244 764 486 693\n",
      "245 125 125 70\n",
      "246 349 349 348\n",
      "247 13 13 18\n",
      "248 179 179 256\n",
      "249 522 522 644\n",
      "Original_Accuracy: 74.0 %\n",
      "Adversarial_Accuracy: 15.0 %\n",
      "150 975 978 977\n",
      "151 620 681 549\n",
      "152 637 637 419\n",
      "153 39 39 42\n",
      "154 115 115 110\n",
      "155 937 937 111\n",
      "156 272 274 225\n",
      "157 277 277 277\n",
      "158 763 763 763\n",
      "159 789 579 975\n",
      "160 646 646 646\n",
      "161 213 234 163\n",
      "162 493 894 619\n",
      "163 647 647 551\n",
      "164 504 504 504\n",
      "165 937 937 973\n",
      "166 687 687 699\n",
      "167 781 781 921\n",
      "168 666 666 419\n",
      "169 583 583 583\n",
      "170 158 158 256\n",
      "171 825 825 700\n",
      "172 212 212 223\n",
      "173 659 659 659\n",
      "174 257 257 257\n",
      "175 436 436 468\n",
      "176 196 199 199\n",
      "177 140 140 142\n",
      "178 248 248 248\n",
      "179 339 339 794\n",
      "180 230 231 255\n",
      "181 361 361 862\n",
      "182 544 909 659\n",
      "183 935 935 961\n",
      "184 638 445 921\n",
      "185 627 656 680\n",
      "186 289 289 794\n",
      "187 867 867 908\n",
      "188 272 272 549\n",
      "189 103 103 632\n",
      "190 584 710 813\n",
      "191 180 180 243\n",
      "192 703 703 693\n",
      "193 449 449 405\n",
      "194 771 771 549\n",
      "195 118 118 78\n",
      "196 396 396 78\n",
      "197 934 711 419\n",
      "198 16 16 733\n",
      "199 548 548 844\n",
      "200 993 993 78\n",
      "201 704 704 794\n",
      "202 457 457 999\n",
      "203 233 233 295\n",
      "204 401 543 419\n",
      "205 827 528 539\n",
      "206 376 376 111\n",
      "207 146 82 913\n",
      "208 606 606 419\n",
      "209 922 922 921\n",
      "210 516 431 516\n",
      "211 284 284 264\n",
      "212 889 889 432\n",
      "213 475 475 405\n",
      "214 978 978 395\n",
      "215 475 475 318\n",
      "216 984 984 984\n",
      "217 16 16 313\n",
      "218 77 77 78\n",
      "219 610 610 610\n",
      "220 254 254 163\n",
      "221 636 636 636\n",
      "222 662 662 419\n",
      "223 473 473 419\n",
      "224 213 207 226\n",
      "225 25 25 650\n",
      "226 463 522 463\n",
      "227 215 215 247\n",
      "228 173 537 273\n",
      "229 35 35 727\n",
      "230 741 741 646\n",
      "231 125 125 908\n",
      "232 787 849 980\n",
      "233 289 289 794\n",
      "234 425 425 646\n",
      "235 973 973 950\n",
      "236 1 115 749\n",
      "237 167 167 167\n",
      "238 121 121 419\n",
      "239 445 876 34\n",
      "240 702 422 416\n",
      "241 532 532 428\n",
      "242 366 294 354\n",
      "243 678 678 678\n",
      "244 764 486 693\n",
      "245 125 125 70\n",
      "246 349 349 348\n",
      "247 13 13 18\n",
      "248 179 179 256\n",
      "249 522 522 883\n",
      "Original_Accuracy: 74.0 %\n",
      "Adversarial_Accuracy: 15.0 %\n",
      "150 975 978 977\n",
      "151 620 681 549\n",
      "152 637 637 419\n",
      "153 39 39 913\n",
      "154 115 115 110\n",
      "155 937 937 111\n",
      "156 272 274 921\n",
      "157 277 277 277\n",
      "158 763 763 763\n",
      "159 789 579 975\n",
      "160 646 646 646\n",
      "161 213 234 163\n",
      "162 493 894 619\n",
      "163 647 647 551\n",
      "164 504 504 504\n",
      "165 937 937 973\n",
      "166 687 687 699\n",
      "167 781 781 921\n",
      "168 666 666 417\n",
      "169 583 583 583\n",
      "170 158 158 256\n",
      "171 825 825 700\n",
      "172 212 212 223\n",
      "173 659 659 659\n",
      "174 257 257 257\n",
      "175 436 436 625\n",
      "176 196 199 199\n",
      "177 140 140 142\n",
      "178 248 248 921\n",
      "179 339 339 794\n",
      "180 230 231 255\n",
      "181 361 361 683\n",
      "182 544 909 659\n",
      "183 935 935 961\n",
      "184 638 445 921\n",
      "185 627 656 619\n",
      "186 289 289 794\n",
      "187 867 867 908\n",
      "188 272 272 549\n",
      "189 103 103 632\n",
      "190 584 710 813\n",
      "191 180 180 243\n",
      "192 703 703 693\n",
      "193 449 449 976\n",
      "194 771 771 549\n",
      "195 118 118 78\n",
      "196 396 396 78\n",
      "197 934 711 419\n",
      "198 16 16 46\n",
      "199 548 548 844\n",
      "200 993 993 78\n",
      "201 704 704 794\n",
      "202 457 457 999\n",
      "203 233 233 646\n",
      "204 401 543 419\n",
      "205 827 528 539\n",
      "206 376 376 111\n",
      "207 146 82 913\n",
      "208 606 606 419\n",
      "209 922 922 921\n",
      "210 516 431 434\n",
      "211 284 284 223\n",
      "212 889 889 693\n",
      "213 475 475 646\n",
      "214 978 978 395\n",
      "215 475 475 318\n",
      "216 984 984 984\n",
      "217 16 16 313\n",
      "218 77 77 78\n",
      "219 610 610 610\n",
      "220 254 254 163\n",
      "221 636 636 636\n",
      "222 662 662 700\n",
      "223 473 473 419\n",
      "224 213 207 226\n",
      "225 25 25 650\n",
      "226 463 522 463\n",
      "227 215 215 247\n",
      "228 173 537 273\n",
      "229 35 35 417\n",
      "230 741 741 646\n",
      "231 125 125 795\n",
      "232 787 849 980\n",
      "233 289 289 794\n",
      "234 425 425 419\n",
      "235 973 973 78\n",
      "236 1 115 749\n",
      "237 167 167 167\n",
      "238 121 121 795\n",
      "239 445 876 646\n",
      "240 702 422 416\n",
      "241 532 532 221\n",
      "242 366 294 354\n",
      "243 678 678 678\n",
      "244 764 486 693\n",
      "245 125 125 616\n",
      "246 349 349 348\n",
      "247 13 13 18\n",
      "248 179 179 256\n",
      "249 522 522 619\n",
      "Original_Accuracy: 74.0 %\n",
      "Adversarial_Accuracy: 13.0 %\n",
      "150 975 978 977\n",
      "151 620 681 45\n",
      "152 637 637 620\n",
      "153 39 39 41\n",
      "154 115 115 110\n",
      "155 937 937 111\n",
      "156 272 274 221\n",
      "157 277 277 109\n",
      "158 763 763 763\n",
      "159 789 579 852\n",
      "160 646 646 646\n",
      "161 213 234 294\n",
      "162 493 894 619\n",
      "163 647 647 111\n",
      "164 504 504 419\n",
      "165 937 937 973\n",
      "166 687 687 699\n",
      "167 781 781 921\n",
      "168 666 666 417\n",
      "169 583 583 583\n",
      "170 158 158 256\n",
      "171 825 825 549\n",
      "172 212 212 223\n",
      "173 659 659 659\n",
      "174 257 257 249\n",
      "175 436 436 625\n",
      "176 196 199 199\n",
      "177 140 140 133\n",
      "178 248 248 921\n",
      "179 339 339 794\n",
      "180 230 231 256\n",
      "181 361 361 683\n",
      "182 544 909 659\n",
      "183 935 935 646\n",
      "184 638 445 446\n",
      "185 627 656 619\n",
      "186 289 289 794\n",
      "187 867 867 908\n",
      "188 272 272 549\n",
      "189 103 103 632\n",
      "190 584 710 584\n",
      "191 180 180 182\n",
      "192 703 703 693\n",
      "193 449 449 976\n",
      "194 771 771 549\n",
      "195 118 118 119\n",
      "196 396 396 78\n",
      "197 934 711 419\n",
      "198 16 16 616\n",
      "199 548 548 620\n",
      "200 993 993 78\n",
      "201 704 704 794\n",
      "202 457 457 999\n",
      "203 233 233 646\n",
      "204 401 543 419\n",
      "205 827 528 539\n",
      "206 376 376 111\n",
      "207 146 82 913\n",
      "208 606 606 419\n",
      "209 922 922 921\n",
      "210 516 431 434\n",
      "211 284 284 490\n",
      "212 889 889 921\n",
      "213 475 475 646\n",
      "214 978 978 395\n",
      "215 475 475 318\n",
      "216 984 984 984\n",
      "217 16 16 313\n",
      "218 77 77 79\n",
      "219 610 610 610\n",
      "220 254 254 256\n",
      "221 636 636 636\n",
      "222 662 662 682\n",
      "223 473 473 419\n",
      "224 213 207 226\n",
      "225 25 25 25\n",
      "226 463 522 646\n",
      "227 215 215 539\n",
      "228 173 537 273\n",
      "229 35 35 900\n",
      "230 741 741 646\n",
      "231 125 125 646\n",
      "232 787 849 980\n",
      "233 289 289 824\n",
      "234 425 425 419\n",
      "235 973 973 78\n",
      "236 1 115 749\n",
      "237 167 167 721\n",
      "238 121 121 700\n",
      "239 445 876 646\n",
      "240 702 422 908\n",
      "241 532 532 221\n",
      "242 366 294 336\n",
      "243 678 678 801\n",
      "244 764 486 693\n",
      "245 125 125 616\n",
      "246 349 349 794\n",
      "247 13 13 620\n",
      "248 179 179 256\n",
      "249 522 522 659\n",
      "Original_Accuracy: 74.0 %\n",
      "Adversarial_Accuracy: 9.0 %\n",
      "150 975 978 977\n",
      "151 620 681 45\n",
      "152 637 637 620\n",
      "153 39 39 45\n",
      "154 115 115 110\n",
      "155 937 937 111\n",
      "156 272 274 327\n",
      "157 277 277 599\n",
      "158 763 763 763\n",
      "159 789 579 646\n",
      "160 646 646 646\n",
      "161 213 234 294\n",
      "162 493 894 619\n",
      "163 647 647 111\n",
      "164 504 504 419\n",
      "165 937 937 973\n",
      "166 687 687 753\n",
      "167 781 781 921\n",
      "168 666 666 419\n",
      "169 583 583 733\n",
      "170 158 158 256\n",
      "171 825 825 78\n",
      "172 212 212 794\n",
      "173 659 659 980\n",
      "174 257 257 327\n",
      "175 436 436 768\n",
      "176 196 199 223\n",
      "177 140 140 133\n",
      "178 248 248 921\n",
      "179 339 339 646\n",
      "180 230 231 256\n",
      "181 361 361 646\n",
      "182 544 909 659\n",
      "183 935 935 646\n",
      "184 638 445 446\n",
      "185 627 656 721\n",
      "186 289 289 794\n",
      "187 867 867 908\n",
      "188 272 272 549\n",
      "189 103 103 632\n",
      "190 584 710 584\n",
      "191 180 180 182\n",
      "192 703 703 693\n",
      "193 449 449 976\n",
      "194 771 771 549\n",
      "195 118 118 119\n",
      "196 396 396 78\n",
      "197 934 711 419\n",
      "198 16 16 616\n",
      "199 548 548 620\n",
      "200 993 993 327\n",
      "201 704 704 794\n",
      "202 457 457 794\n",
      "203 233 233 646\n",
      "204 401 543 419\n",
      "205 827 528 539\n",
      "206 376 376 111\n",
      "207 146 82 78\n",
      "208 606 606 419\n",
      "209 922 922 549\n",
      "210 516 431 419\n",
      "211 284 284 490\n",
      "212 889 889 921\n",
      "213 475 475 646\n",
      "214 978 978 395\n",
      "215 475 475 318\n",
      "216 984 984 984\n",
      "217 16 16 313\n",
      "218 77 77 794\n",
      "219 610 610 714\n",
      "220 254 254 256\n",
      "221 636 636 636\n",
      "222 662 662 682\n",
      "223 473 473 419\n",
      "224 213 207 347\n",
      "225 25 25 25\n",
      "226 463 522 646\n",
      "227 215 215 794\n",
      "228 173 537 294\n",
      "229 35 35 900\n",
      "230 741 741 646\n",
      "231 125 125 908\n",
      "232 787 849 78\n",
      "233 289 289 549\n",
      "234 425 425 419\n",
      "235 973 973 78\n",
      "236 1 115 749\n",
      "237 167 167 646\n",
      "238 121 121 693\n",
      "239 445 876 646\n",
      "240 702 422 733\n",
      "241 532 532 221\n",
      "242 366 294 336\n",
      "243 678 678 983\n",
      "244 764 486 795\n",
      "245 125 125 616\n",
      "246 349 349 794\n",
      "247 13 13 646\n",
      "248 179 179 256\n",
      "249 522 522 619\n",
      "Original_Accuracy: 74.0 %\n",
      "Adversarial_Accuracy: 6.0 %\n"
     ]
    }
   ],
   "source": [
    "min_img_size = 224 \n",
    "fro = 150\n",
    "imgs =  100 # Choose number of images you want to check\n",
    "epsilons = [0.01, 0.05, 0.10, 0.20, 0.30]\n",
    "adv_accuracies = []\n",
    "\n",
    "defender = resnet\n",
    "defender_full = full_resnet\n",
    "\n",
    "attacker = vgg19\n",
    "attacker_full = full_vgg19\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    \n",
    "    original_correct = 0\n",
    "    adv_correct = 0\n",
    "    cls = {}\n",
    "    \n",
    "    for i in range(fro,fro+imgs):\n",
    "        if(i<10):\n",
    "            directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' +'00' +str(i) \n",
    "        elif(i<100):\n",
    "            directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' +'0' +str(i)\n",
    "        elif(i<1000):\n",
    "            directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' + str(i)\n",
    "        filename = directory_imgs \n",
    "        img = Image.open(directory_imgs+'.JPEG')\n",
    "\n",
    "        # Transformation of image\n",
    "        pre_transform = transforms.Compose([transforms.Resize((min_img_size,min_img_size)), transforms.ToTensor(),transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ])\n",
    "        image_tensor = pre_transform(img)\n",
    "        image_tensor = image_tensor.unsqueeze(0) # add batch dimension.  C X H X W ==> B X C X H X W\n",
    "        img_variable = Variable(image_tensor, requires_grad=True) #convert tensor into a variable\n",
    "\n",
    "        # Classification\n",
    "        output = defender.forward(img_variable)\n",
    "        original_class = torch.max(output.data, 1)[1][0]  #get an index(class number) of a largest element\n",
    "        original_class = original_class.item()\n",
    "        cls.update({i:original_class})\n",
    "\n",
    "        # Adversarial attack\n",
    "        adv = fgsm_untargeted(attacker, image_tensor, label=original_class, epsilon = epsilon, clamp=True)\n",
    "        #adv = pgd_untargeted(model, image_tensor, original_class, k=10, epsilon, epsilon_step, clamp=False)\n",
    "        adv = adv.cpu().detach()\n",
    "        probability_adverserial = defender_full(adv.to(device)).cpu().detach().numpy()\n",
    "        adverserial_image = adv.numpy()[0]\n",
    "        prediction = probability_adverserial.argmax()\n",
    "\n",
    "        # Computing Accuracies\n",
    "        if(original_class == Converted_labels[i-1]):\n",
    "            original_correct+=1\n",
    "        if(prediction == Converted_labels[i-1]):\n",
    "            adv_correct+=1\n",
    "\n",
    "        print(i,Converted_labels[i-1],original_class, prediction)\n",
    "\n",
    "\n",
    "    original_correct = original_correct/imgs*100 \n",
    "    adv_correct = adv_correct/imgs*100\n",
    "    print('Original_Accuracy: '+ str(original_correct) + ' %' )\n",
    "    print('Adversarial_Accuracy: '+ str(adv_correct) + str(' %'))\n",
    "    \n",
    "    adv_accuracies.append(adv_correct)\n",
    "\n",
    "epsilons.insert(0,0)\n",
    "adv_accuracies.insert(0,original_correct)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epsilons, adv_accuracies, '*-')\n",
    "plt.title(\"Accuracy percentage Vs epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
