{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import requests\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # I'm not sure if it will work out of the box with cuda even if you have a GPU\n",
    "model = models.vgg16(pretrained=True)\n",
    "full_model = nn.Sequential(model, nn.Softmax(dim=1)).to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is done because the labels for the validation dataset do not match the labels from the pretrained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'labels/imagenet_class_index.json'\n",
    "with open(PATH, 'r') as fp:\n",
    "    labels_65 = json.load(fp)\n",
    "\n",
    "# labels_65 is the default labels from pretrained model\n",
    "labels_65 = {value[0]: int(key) for key,value in labels_65.items()} \n",
    "\n",
    "labels_file = open('labels/labels.txt')\n",
    "labels_490 = {} #labels_490 is the labels from the validation dataset txt file\n",
    "for line in labels_file:\n",
    "    contents = line.split()\n",
    "    val = contents[2]\n",
    "    key = contents[1]\n",
    "    n_name = contents[0] \n",
    "    labels_490.update({int(key): [n_name,val]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_labels_file(k_490):\n",
    "    n_name = labels_490[k_490][0]\n",
    "    k_65 = labels_65[n_name]\n",
    "    \n",
    "    return k_65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_handle = open('labels/ILSVRC2012_validation_ground_truth.txt')\n",
    "\n",
    "count = 0\n",
    "num_images = 1000\n",
    "Converted_labels = []\n",
    "for line in ground_truth_handle:\n",
    "    if count == num_images:\n",
    "        break\n",
    "    k_65 = Convert_labels_file(int(line))\n",
    "    Converted_labels.append(k_65)\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 977 975\n",
      "151 617 620\n",
      "152 637 637\n",
      "153 39 39\n",
      "154 107 115\n",
      "155 108 937\n",
      "156 273 272\n",
      "157 277 277\n",
      "158 763 763\n",
      "159 905 789\n",
      "160 646 646\n",
      "161 207 213\n",
      "162 894 493\n",
      "163 438 647\n",
      "164 504 504\n",
      "165 937 937\n",
      "166 687 687\n",
      "167 781 781\n",
      "168 666 666\n",
      "169 583 583\n",
      "170 158 158\n",
      "171 884 825\n",
      "172 212 212\n",
      "173 659 659\n",
      "174 257 257\n",
      "175 436 436\n",
      "176 199 196\n",
      "177 140 140\n",
      "178 250 248\n",
      "179 728 339\n",
      "180 230 230\n",
      "181 999 361\n",
      "182 659 544\n",
      "183 961 935\n",
      "184 639 638\n",
      "185 627 627\n",
      "186 434 289\n",
      "187 908 867\n",
      "188 270 272\n",
      "189 103 103\n",
      "190 584 584\n",
      "191 208 180\n",
      "192 448 703\n",
      "193 510 449\n",
      "194 771 771\n",
      "195 118 118\n",
      "196 396 396\n",
      "197 470 934\n",
      "198 129 16\n",
      "199 556 548\n",
      "200 8 993\n",
      "201 861 704\n",
      "202 452 457\n",
      "203 233 233\n",
      "204 416 401\n",
      "205 605 827\n",
      "206 6 376\n",
      "207 363 146\n",
      "208 606 606\n",
      "209 922 922\n",
      "210 516 516\n",
      "211 284 284\n",
      "212 731 889\n",
      "213 475 475\n",
      "214 419 978\n",
      "215 861 475\n",
      "216 984 984\n",
      "217 16 16\n",
      "218 77 77\n",
      "219 678 610\n",
      "220 87 254\n",
      "221 636 636\n",
      "222 662 662\n",
      "223 473 473\n",
      "224 220 213\n",
      "225 28 25\n",
      "226 720 463\n",
      "227 212 215\n",
      "228 250 173\n",
      "229 305 35\n",
      "230 741 741\n",
      "231 913 125\n",
      "232 913 787\n",
      "233 289 289\n",
      "234 425 425\n",
      "235 973 973\n",
      "236 126 1\n",
      "237 167 167\n",
      "238 813 121\n",
      "239 876 445\n",
      "240 702 702\n",
      "241 579 532\n",
      "242 295 366\n",
      "243 678 678\n",
      "244 728 764\n",
      "245 419 125\n",
      "246 348 349\n",
      "247 13 13\n",
      "248 180 179\n",
      "249 731 522\n"
     ]
    }
   ],
   "source": [
    "min_img_size = 224 \n",
    "fro = 150\n",
    "imgs = 100\n",
    "cls = {}\n",
    "for i in range(fro,fro+imgs):\n",
    "    if(i<10):\n",
    "        directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' +'00' +str(i) \n",
    "    elif(i<100):\n",
    "        directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' +'0' +str(i)\n",
    "    elif(i<1000):\n",
    "        directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' + str(i)\n",
    "        \n",
    "    filename = directory_imgs \n",
    "    img = Image.open(directory_imgs+'.JPEG')\n",
    "    pre_transform = transforms.Compose([transforms.Resize((min_img_size,min_img_size))])\n",
    "    img = pre_transform(img)\n",
    "    original = transforms.ToTensor()(img).to(device)[None,]\n",
    "    original_class = model(original).detach().cpu().numpy().argmax()\n",
    "    cls.update({i:original_class})\n",
    "    \n",
    "    print(i,original_class,Converted_labels[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43564356435643564\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(fro,fro+imgs):\n",
    "    predicted_val = cls[i]\n",
    "    true_val = Converted_labels[i-1]\n",
    "    if true_val == predicted_val:\n",
    "        correct+=1\n",
    "accuracy = correct/(imgs+1)\n",
    "print(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __fgsm(model, x, label, epsilon, targeted, clamp):\n",
    "    model.zero_grad()\n",
    "\n",
    "    x = torch.as_tensor(x, device=device)\n",
    "    x.requires_grad = True\n",
    "    \n",
    "    logits = model(x)\n",
    "    target = torch.LongTensor([label]).to(device)\n",
    "    loss = nn.CrossEntropyLoss()(logits, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    diff = epsilon * x.grad.sign()\n",
    "    \n",
    "    if targeted:\n",
    "        adv = x - diff\n",
    "    else:\n",
    "        adv = x + diff\n",
    "    \n",
    "    return adv.clamp(0, 1) if clamp else adv\n",
    "\n",
    "def fgsm_targeted(model, x, label, epsilon, clamp=True):\n",
    "    \"\"\"\n",
    "    model = neural network with logits output (not softmax)\n",
    "    x = image \n",
    "    label = target label (don't care about the true label)\n",
    "    epsilon = distance to move \n",
    "    clamp = Limit output values to the range [0,1]\n",
    "    \"\"\"\n",
    "    return __fgsm(model, x, label, epsilon, True, clamp)\n",
    "    \n",
    "def fgsm_untargeted(model, x, label, epsilon, clamp=True):    \n",
    "    \"\"\"\n",
    "    model = neural network with logits output (not softmax)\n",
    "    x = image\n",
    "    label = the true label\n",
    "    epsilon = distance to move \n",
    "    clamp = Limit output values to the range [0,1]\n",
    "    \"\"\"\n",
    "    return __fgsm(model, x, label, epsilon, False, clamp)\n",
    "\n",
    "def plotter(image, adverserial, full_model):\n",
    "    \"\"\"Show the original (img), adversarial attack (adv), and some more stuff. Pass images as pytorch tensors\"\"\"\n",
    "    image = image.cpu().detach()\n",
    "    adverserial = adverserial.cpu().detach()\n",
    "    \n",
    "    probability_original = full_model(image.to(device)).cpu().detach().numpy()\n",
    "    origial_image = image.numpy()[0]\n",
    "    \n",
    "    probability_adverserial = full_model(adverserial.to(device)).cpu().detach().numpy()\n",
    "    adverserial_image = adverserial.numpy()[0]\n",
    "    \n",
    "    f, axes = plt.subplots(1,3, figsize=(15, 6))\n",
    "    \n",
    "    ax = axes[0]\n",
    "    prediction = probability_original.argmax()\n",
    "    ax.set_title(\"Original, class: {} ({:.0f} %): '{}'\".format(\n",
    "        prediction, 100*probability_original[0, prediction], labels[prediction])\n",
    "    )\n",
    "    ax.imshow(np.transpose(origial_image, (1,2,0)))\n",
    "    \n",
    "    ax = axes[1]\n",
    "    prediction = probability_adverserial.argmax()\n",
    "    ax.set_title(\"Adverserial, class: {} ({:.0f} %): '{}'\".format(\n",
    "        prediction, 100*probability_adverserial[0, prediction], labels[prediction])\n",
    "    )\n",
    "    ax.imshow(np.transpose(adverserial_image, (1,2,0)))\n",
    "    \n",
    "    ax = axes[2]\n",
    "    diff = np.transpose(adverserial_image - origial_image, (1,2,0))\n",
    "    plt.imshow(diff + 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 100 validation images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 975 977 978\n",
      "151 620 617 620\n",
      "152 637 637 902\n",
      "153 39 39 47\n",
      "154 115 107 397\n",
      "155 937 108 669\n",
      "156 272 273 343\n",
      "157 277 277 279\n",
      "158 763 763 596\n",
      "159 789 905 765\n",
      "160 646 646 970\n",
      "161 213 207 161\n",
      "162 493 894 861\n",
      "163 647 438 647\n",
      "164 504 504 669\n",
      "165 937 937 109\n",
      "166 687 687 753\n",
      "167 781 781 922\n",
      "168 666 666 659\n",
      "169 583 583 669\n",
      "Original_Accuracy: 55.00000000000001 %\n",
      "Adversarial_Accuracy: 10.0 %\n"
     ]
    }
   ],
   "source": [
    "min_img_size = 224 \n",
    "fro = 150\n",
    "imgs =  # Choose number of images you want to check\n",
    "original_correct = 0\n",
    "adv_correct = 0\n",
    "cls = {}\n",
    "\n",
    "for i in range(fro,fro+imgs):\n",
    "    if(i<10):\n",
    "        directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' +'00' +str(i) \n",
    "    elif(i<100):\n",
    "        directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' +'0' +str(i)\n",
    "    elif(i<1000):\n",
    "        directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' + str(i)\n",
    "        \n",
    "    filename = directory_imgs \n",
    "    img = Image.open(directory_imgs+'.JPEG')\n",
    "    pre_transform = transforms.Compose([transforms.Resize((min_img_size,min_img_size))])\n",
    "    img = pre_transform(img)\n",
    "    original = transforms.ToTensor()(img).to(device)[None,]\n",
    "    original_class = model(original).detach().cpu().numpy().argmax()\n",
    "    cls.update({i:original_class})\n",
    "    \n",
    "    adv = fgsm_untargeted(model, original, label=original_class, epsilon=0.01, clamp=True)\n",
    "    \n",
    "    \n",
    "    original = original.cpu().detach()\n",
    "    adv = adv.cpu().detach()\n",
    "    \n",
    "    probability_original = full_model(original.to(device)).cpu().detach().numpy()\n",
    "    origial_image = original.numpy()[0]\n",
    "    \n",
    "    probability_adverserial = full_model(adv.to(device)).cpu().detach().numpy()\n",
    "    adverserial_image = adv.numpy()[0]\n",
    "    \n",
    "    prediction = probability_adverserial.argmax()\n",
    "    \n",
    "    print(i,Converted_labels[i-1],original_class, prediction)\n",
    "    \n",
    "    if(original_class == Converted_labels[i-1]):\n",
    "        original_correct+=1\n",
    "    if(prediction == Converted_labels[i-1]):\n",
    "        adv_correct+=1\n",
    "\n",
    "original_correct = original_correct/imgs*100 \n",
    "adv_correct = adv_correct/imgs*100\n",
    "print('Original_Accuracy: '+ str(original_correct) + ' %' )\n",
    "print('Adversarial_Accuracy: '+ str(adv_correct) + str(' %'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
