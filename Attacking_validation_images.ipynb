{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import requests\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # I'm not sure if it will work out of the box with cuda even if you have a GPU\n",
    "model = models.vgg19(pretrained=True)\n",
    "full_model = nn.Sequential(model, nn.Softmax(dim=1)).to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is done because the labels for the validation dataset do not match the labels from the pretrained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'labels/imagenet_class_index.json'\n",
    "with open(PATH, 'r') as fp:\n",
    "    labels_65 = json.load(fp)\n",
    "\n",
    "# labels_65 is the default labels from pretrained model\n",
    "labels_65 = {value[0]: int(key) for key,value in labels_65.items()} \n",
    "\n",
    "labels_file = open('labels/labels.txt')\n",
    "labels_490 = {} #labels_490 is the labels from the validation txt\n",
    "for line in labels_file:\n",
    "    contents = line.split()\n",
    "    val = contents[2]\n",
    "    key = contents[1]\n",
    "    n_name = contents[0] \n",
    "    labels_490.update({int(key): [n_name,val]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_labels_file(k_490):\n",
    "    n_name = labels_490[k_490][0]\n",
    "    k_65 = labels_65[n_name]\n",
    "    \n",
    "    return k_65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_handle = open('labels/ILSVRC2012_validation_ground_truth.txt')\n",
    "\n",
    "count = 0\n",
    "num_images = 1000\n",
    "Converted_labels = []\n",
    "for line in ground_truth_handle:\n",
    "    if count == num_images:\n",
    "        break\n",
    "    k_65 = Convert_labels_file(int(line))\n",
    "    Converted_labels.append(k_65)\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __fgsm(model, x, label, epsilon, targeted, clamp):\n",
    "    model.zero_grad()\n",
    "\n",
    "    x = torch.as_tensor(x, device=device)\n",
    "    x.requires_grad = True\n",
    "    \n",
    "    logits = model(x)\n",
    "    target = torch.LongTensor([label]).to(device)\n",
    "    loss = nn.CrossEntropyLoss()(logits, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    diff = epsilon * x.grad.sign()\n",
    "    \n",
    "    if targeted:\n",
    "        adv = x - diff\n",
    "    else:\n",
    "        adv = x + diff\n",
    "    \n",
    "    return adv.clamp(0, 1) if clamp else adv\n",
    "\n",
    "def fgsm_targeted(model, x, label, epsilon, clamp=True):\n",
    "    \"\"\"\n",
    "    model = neural network with logits output (not softmax)\n",
    "    x = image \n",
    "    label = target label (don't care about the true label)\n",
    "    epsilon = distance to move \n",
    "    clamp = Limit output values to the range [0,1]\n",
    "    \"\"\"\n",
    "    return __fgsm(model, x, label, epsilon, True, clamp)\n",
    "    \n",
    "def fgsm_untargeted(model, x, label, epsilon, clamp=True):    \n",
    "    \"\"\"\n",
    "    model = neural network with logits output (not softmax)\n",
    "    x = image\n",
    "    label = the true label\n",
    "    epsilon = distance to move \n",
    "    clamp = Limit output values to the range [0,1]\n",
    "    \"\"\"\n",
    "    return __fgsm(model, x, label, epsilon, False, clamp)\n",
    "\n",
    "def plotter(image, adverserial, full_model):\n",
    "    \"\"\"Show the original (img), adversarial attack (adv), and some more stuff. Pass images as pytorch tensors\"\"\"\n",
    "    image = image.cpu().detach()\n",
    "    adverserial = adverserial.cpu().detach()\n",
    "    \n",
    "    probability_original = full_model(image.to(device)).cpu().detach().numpy()\n",
    "    origial_image = image.numpy()[0]\n",
    "    \n",
    "    probability_adverserial = full_model(adverserial.to(device)).cpu().detach().numpy()\n",
    "    adverserial_image = adverserial.numpy()[0]\n",
    "    \n",
    "    f, axes = plt.subplots(1,3, figsize=(15, 6))\n",
    "    \n",
    "    ax = axes[0]\n",
    "    prediction = probability_original.argmax()\n",
    "    ax.set_title(\"Original, class: {} ({:.0f} %): '{}'\".format(\n",
    "        prediction, 100*probability_original[0, prediction], labels[prediction])\n",
    "    )\n",
    "    ax.imshow(np.transpose(origial_image, (1,2,0)))\n",
    "    \n",
    "    ax = axes[1]\n",
    "    prediction = probability_adverserial.argmax()\n",
    "    ax.set_title(\"Adverserial, class: {} ({:.0f} %): '{}'\".format(\n",
    "        prediction, 100*probability_adverserial[0, prediction], labels[prediction])\n",
    "    )\n",
    "    ax.imshow(np.transpose(adverserial_image, (1,2,0)))\n",
    "    \n",
    "    ax = axes[2]\n",
    "    diff = np.transpose(adverserial_image - origial_image, (1,2,0))\n",
    "    plt.imshow(diff + 0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack 100 validation images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 975 977 978\n",
      "151 620 922 611\n",
      "152 637 637 730\n",
      "153 39 39 47\n",
      "154 115 115 107\n",
      "155 937 937 111\n",
      "156 272 274 980\n",
      "157 277 277 5\n",
      "158 763 763 763\n",
      "159 789 905 765\n",
      "160 646 646 646\n",
      "161 213 214 980\n",
      "162 493 894 799\n",
      "163 647 647 438\n",
      "164 504 504 504\n",
      "165 937 937 973\n",
      "166 687 687 716\n",
      "167 781 781 922\n",
      "168 666 666 659\n",
      "169 583 583 583\n",
      "170 158 158 107\n",
      "171 825 825 825\n",
      "172 212 212 251\n",
      "173 659 659 659\n",
      "174 257 257 222\n",
      "175 436 436 436\n",
      "176 196 199 110\n",
      "177 140 140 6\n",
      "178 248 250 282\n",
      "179 339 339 921\n",
      "180 230 230 6\n",
      "181 361 731 862\n",
      "182 544 926 868\n",
      "183 935 935 961\n",
      "184 638 445 921\n",
      "185 627 656 656\n",
      "186 289 289 815\n",
      "187 867 867 908\n",
      "188 272 272 353\n",
      "189 103 979 111\n",
      "190 584 823 892\n",
      "191 180 179 921\n",
      "192 703 448 693\n",
      "193 449 449 510\n",
      "194 771 771 446\n",
      "195 118 118 121\n",
      "196 396 396 396\n",
      "197 934 922 549\n",
      "198 16 16 989\n",
      "199 548 548 556\n",
      "200 993 993 973\n",
      "201 704 704 438\n",
      "202 457 823 411\n",
      "203 233 233 295\n",
      "204 401 543 549\n",
      "205 827 482 605\n",
      "206 376 376 4\n",
      "207 146 85 815\n",
      "208 606 606 662\n",
      "209 922 922 782\n",
      "210 516 431 793\n",
      "211 284 284 264\n",
      "212 889 889 921\n",
      "213 475 475 475\n",
      "214 978 723 895\n",
      "215 475 475 405\n",
      "216 984 984 984\n",
      "217 16 16 21\n",
      "218 77 77 815\n",
      "219 610 610 433\n",
      "220 254 254 6\n",
      "221 636 636 636\n",
      "222 662 662 647\n",
      "223 473 473 596\n",
      "224 213 207 213\n",
      "225 25 25 611\n",
      "226 463 427 417\n",
      "227 215 215 434\n",
      "228 173 537 921\n",
      "229 35 35 347\n",
      "230 741 741 611\n",
      "231 125 125 2\n",
      "232 787 562 983\n",
      "233 289 289 973\n",
      "234 425 425 611\n",
      "235 973 973 973\n",
      "236 1 115 115\n",
      "237 167 167 162\n",
      "238 121 121 4\n",
      "239 445 562 611\n",
      "240 702 422 602\n",
      "241 532 532 619\n",
      "242 366 294 295\n",
      "243 678 678 678\n",
      "244 764 650 126\n",
      "245 125 113 419\n",
      "246 349 348 348\n",
      "247 13 13 10\n",
      "248 179 179 295\n",
      "249 522 522 794\n",
      "Original_Accuracy: 66.0 %\n",
      "Adversarial_Accuracy: 14.000000000000002 %\n"
     ]
    }
   ],
   "source": [
    "min_img_size = 224 \n",
    "fro = 150\n",
    "imgs =  100 # Choose number of images you want to check\n",
    "original_correct = 0\n",
    "adv_correct = 0\n",
    "cls = {}\n",
    "\n",
    "for i in range(fro,fro+imgs):\n",
    "    if(i<10):\n",
    "        directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' +'00' +str(i) \n",
    "    elif(i<100):\n",
    "        directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' +'0' +str(i)\n",
    "    elif(i<1000):\n",
    "        directory_imgs = 'ILSVRC2012_img_val/ILSVRC2012_val_00000' + str(i)\n",
    "    filename = directory_imgs \n",
    "    img = Image.open(directory_imgs+'.JPEG')\n",
    "    \n",
    "    # Transformation of image\n",
    "    pre_transform = transforms.Compose([transforms.Resize((min_img_size,min_img_size)), transforms.ToTensor(),transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ])\n",
    "    image_tensor = pre_transform(img)\n",
    "    image_tensor = image_tensor.unsqueeze(0) # add batch dimension.  C X H X W ==> B X C X H X W\n",
    "    img_variable = Variable(image_tensor, requires_grad=True) #convert tensor into a variable\n",
    "    \n",
    "    # Classification\n",
    "    output = model.forward(img_variable)\n",
    "    original_class = torch.max(output.data, 1)[1][0]  #get an index(class number) of a largest element\n",
    "    original_class = original_class.item()\n",
    "    cls.update({i:original_class})\n",
    "    \n",
    "    # Adversarial attack\n",
    "    adv = fgsm_untargeted(model, image_tensor, label=original_class, epsilon=0.01, clamp=True)\n",
    "    adv = adv.cpu().detach()\n",
    "    probability_adverserial = full_model(adv.to(device)).cpu().detach().numpy()\n",
    "    adverserial_image = adv.numpy()[0]\n",
    "    prediction = probability_adverserial.argmax()\n",
    "        \n",
    "    # Computing Accuracies\n",
    "    if(original_class == Converted_labels[i-1]):\n",
    "        original_correct+=1\n",
    "    if(prediction == Converted_labels[i-1]):\n",
    "        adv_correct+=1\n",
    "        \n",
    "    print(i,Converted_labels[i-1],original_class, prediction)\n",
    "\n",
    "\n",
    "original_correct = original_correct/imgs*100 \n",
    "adv_correct = adv_correct/imgs*100\n",
    "print('Original_Accuracy: '+ str(original_correct) + ' %' )\n",
    "print('Adversarial_Accuracy: '+ str(adv_correct) + str(' %'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
